{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beQ8MgHaYqwJ",
        "outputId": "660ec154-f027-48b1-ddb6-8ec5e79c7e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 122 ms, sys: 27.1 ms, total: 149 ms\n",
            "Wall time: 16.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%pip install -q -U langchain\n",
        "%pip install -q -U langchain-google-genai\n",
        "%pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aul4iUhDab2w",
        "outputId": "b33106e0-64f2-48b6-95df-4c82013f178e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3wv7-7wQXzWT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "genminiapikey = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GEMINI_API_KEY'] = genminiapikey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJHTnsYjd3KD"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "llm : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-1.5-flash\",\n",
        "    api_key = genminiapikey,\n",
        "    temperature = 0.2\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LysN2hm7VrKP"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ['GEMINI_API_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "BQbzbvGFgXTT",
        "outputId": "202b782b-2f2b-4194-eb16-ae324d5dcc65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Pro 002',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Flash 002',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: want to check llms models how many models are there\n",
        "\n",
        "from langchain_google_genai import llms\n",
        "\n",
        "list(llms.genai.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5pXU8yij4_gp",
        "outputId": "b8dca729-1e83-4d51-9272-b492ed5cc6e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Any',\n",
              " 'AsyncCallbackManagerForLLMRun',\n",
              " 'BaseLLM',\n",
              " 'BaseModel',\n",
              " 'Callable',\n",
              " 'CallbackManagerForLLMRun',\n",
              " 'ConfigDict',\n",
              " 'Dict',\n",
              " 'Enum',\n",
              " 'Field',\n",
              " 'Generation',\n",
              " 'GenerationChunk',\n",
              " 'GoogleGenerativeAI',\n",
              " 'GoogleModelFamily',\n",
              " 'HarmBlockThreshold',\n",
              " 'HarmCategory',\n",
              " 'Iterator',\n",
              " 'LLMResult',\n",
              " 'LangSmithParams',\n",
              " 'LanguageModelInput',\n",
              " 'List',\n",
              " 'Optional',\n",
              " 'SecretStr',\n",
              " 'Self',\n",
              " 'Union',\n",
              " '_BaseGoogleGenerativeAI',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " '_completion_with_retry',\n",
              " '_create_retry_decorator',\n",
              " '_strip_erroneous_leading_spaces',\n",
              " 'annotations',\n",
              " 'auto',\n",
              " 'create_base_retry_decorator',\n",
              " 'genai',\n",
              " 'google',\n",
              " 'model_validator',\n",
              " 'secret_from_env']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(llms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PJCRWB6CYR6c"
      },
      "outputs": [],
      "source": [
        "for m in llms.genai.list_models():\n",
        "    if \"generateContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQR583UcjhZk"
      },
      "outputs": [],
      "source": [
        "aimessage : AIMessage = llm.invoke(\"what is the capital of france\")\n",
        "\n",
        "print(aimessage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGNLRzDNmqS7"
      },
      "outputs": [],
      "source": [
        "aimessage.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "40BcK85hpRN9"
      },
      "outputs": [],
      "source": [
        "message : AIMessage = llm.invoke(\"I want to open a resturant for Indian food, suggest a fancy name for this\")\n",
        "print(message)\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w37JJRAZsBYV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "\n",
        "from IPython.display import display , Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace(\"â€¢\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg-sxnmqsLd8"
      },
      "outputs": [],
      "source": [
        "to_markdown(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxH9uOd4rAz"
      },
      "source": [
        "**Prompt Template and Chain**\n",
        "\n",
        "---\n",
        "\n",
        "# prompt template\n",
        "**Prompt Template**\n",
        "A prompt template is a predefined structure for generating prompts that will be sent to a language model. It defines how the input should be formatted, including placeholders for dynamic values. This helps create consistent and context-aware inputs for the model.\n",
        "\n",
        "# Chain\n",
        "\n",
        "**Chain**\n",
        "A chain in LangChain is a sequence of operations that connects different components to perform a task. For example, a chain might take user input, process it with a language model, and return a response. It helps organize the flow of data and actions in your application.\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "Chain:\n",
        "\n",
        "*   Input: User question\n",
        "*   Process: Language model generates a response\n",
        "*   Output: Response is returned to the user\n",
        "\n",
        "Prompt Template:\n",
        "\n",
        "* Template: \"User: {user_input}\\nAI:\"\n",
        "* Usage: If user_input is \"What's the weather?\", the prompt becomes:\n",
        "   * \"User: What's the weather?\\nAI:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIyVR5bqscBH"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompttemplate_resturant = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"I want to open a resturant for {product} food, suggest a fancy name for this only 5\"\n",
        ")\n",
        "\n",
        "prompttemplate_resturant.format(product=\"italian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQj5Ff-W04lc"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain_resturant = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = prompttemplate_resturant\n",
        ")\n",
        "\n",
        "print(chain_resturant.run(\"italian\"))\n",
        "print(to_markdown(chain_resturant.run(\"italian\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mRU7D7B58T2"
      },
      "source": [
        "# Simple Sequential Chain\n",
        "\n",
        "A Simple sequential chain in LangChain refers to a specific type of chain where tasks are executed one after the other in a predetermined order. Each step in the sequence depends on the output of the previous step. This approach is useful when you need to perform multiple operations in a specific order, such as processing user input, generating responses, and then perhaps formatting or saving the output.\n",
        "\n",
        "**How Sequential Chains Work**\n",
        "\n",
        "\n",
        "\n",
        "1.   Input: The process starts with an initial input (e.g., a user question).\n",
        "2.   Processing Steps: Each step takes the output from the previous step as its input. For example:\n",
        "  *   Step 1: Process the user input (cleaning, formatting, etc.).\n",
        "  *   Step 2: Generate a response using a language model.\n",
        "  * Step 3: Format or log the response.\n",
        "3.  Output: The final output is produced after all steps have been executed\n",
        "\n",
        "\n",
        "\n",
        "**`Where the input of 2nd step is the output of the first step but it give u the one output `**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSMW7b_l56Kl"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-1.5-flash\",\n",
        "    api_key = genminiapikey,\n",
        "    temperature = 0.5\n",
        ")\n",
        "\n",
        "prompttemplate_resturant = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"I want to open a resturant for {product} food, suggest a fancy name for this only 5\"\n",
        ")\n",
        "\n",
        "\n",
        "chain_resturant = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = prompttemplate_resturant\n",
        ")\n",
        "# prompttemplate_resturant.format(product=\"italian\")\n",
        "\n",
        "prompttemplate_menueitem = PromptTemplate(\n",
        "    input_variables=[\"menueItems\"],\n",
        "    template = \"suggest some menue item for this resturant {menueItem}\"\n",
        ")\n",
        "\n",
        "chain_menueitems = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompttemplate_menueitem\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U02mdvRe87Ai"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "SimpleSequentialChain_resturant_menueitems = SimpleSequentialChain(\n",
        "    chains=[chain_resturant,chain_menueitems],\n",
        ")\n",
        "SimpleSequentialChain_resturant_menueitems.run(\"india\")\n",
        "\n",
        "to_markdown(SimpleSequentialChain_resturant_menueitems.run(\"indian\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CidjQNBw_mSQ"
      },
      "source": [
        "# Sequential Chain\n",
        "\n",
        "A **Sequential Chain** in LangChain is a more advanced type of chain that allows for executing multiple tasks in a specific order, with each step potentially handling multiple inputs and outputs. This approach is particularly useful when you need to manage complex workflows that require various operations to be performed in a sequence, where later steps might depend on the outputs of earlier ones.\n",
        "\n",
        "**How Sequential Chains Work**\n",
        "\n",
        "1. **Input**: The process begins with an initial input (e.g., a user query).\n",
        "2. **Processing Steps**: Each step can process the output of the previous step and might also take additional inputs. For example:\n",
        "   - **Step 1**: Process the user input (e.g., cleaning or formatting).\n",
        "   - **Step 2**: Generate a response using a language model.\n",
        "   - **Step 3**: Format or aggregate outputs from multiple sources.\n",
        "3. **Output**: The final output is produced after all steps have been executed, which can include various results from different steps.\n",
        "\n",
        "**Key Features**:\n",
        "- Each step can receive multiple inputs and return multiple outputs.\n",
        "- Allows for greater flexibility and complexity compared to a Simple Sequential Chain.\n",
        "- Suitable for workflows that require conditional logic or aggregation of data.\n",
        "\n",
        "**`Where the inputs and outputs of each step can vary, allowing for more complex interactions.`**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwcXZNBI_pBz"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-1.5-flash\",\n",
        "    api_key = genminiapikey,\n",
        "    temperature = 0.5\n",
        ")\n",
        "\n",
        "prompttemplate_resturant = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"I want to open a resturant for {product} food, suggest a fancy name for this only 5\"\n",
        ")\n",
        "\n",
        "\n",
        "chain_resturant = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = prompttemplate_resturant,\n",
        "    output_key=\"resturant_name\"\n",
        ")\n",
        "# prompttemplate_resturant.format(product=\"italian\")\n",
        "\n",
        "prompttemplate_menueitem = PromptTemplate(\n",
        "    input_variables=[\"resturant_name\"],\n",
        "    template = \"suggest some menue item for this resturant {resturant_name}\"\n",
        ")\n",
        "\n",
        "chain_menueitems = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompttemplate_menueitem,\n",
        "    output_key=\"menue_items\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4rBicolRHnxq"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "\n",
        "sequentialchain_resturant_menueitems = SequentialChain(\n",
        "    chains=[chain_resturant,chain_menueitems],\n",
        "    input_variables=[\"product\"],\n",
        "    output_variables=[\"resturant_name\",\"menue_items\"],\n",
        "    verbose=True\n",
        ")\n",
        "result = sequentialchain_resturant_menueitems.invoke({\"product\":\"indian\"})\n",
        "\n",
        "\n",
        "output_string = f\"Restaurant Name: {result['resturant_name']}\\nMenu Items: {result['menue_items']}\"\n",
        "\n",
        "print(output_string)\n",
        "\n",
        "to_markdown(output_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYA3qzQ9sPDD"
      },
      "outputs": [],
      "source": [
        "from langchian.agents import AgentType, initialize_agent,load_tools\n",
        "\n",
        "tools = load_tools([\"wikipedia\",\"llm-math\"],llm=llm)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True)\n",
        "\n",
        "agent.run(\"when was elon musk was born\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}